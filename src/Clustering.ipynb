{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d688617a",
   "metadata": {},
   "source": [
    "# Hierarchical clustering program using different distance measures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "286f1b4c",
   "metadata": {},
   "source": [
    "Use pm4py for importing different event logs found in logs folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba77fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import pprint\n",
    "\n",
    "def get_log(path):\n",
    "    '''\n",
    "    Reads in an event log from the logs folder using pm4py \n",
    "    '''\n",
    "\n",
    "    return pm4py.read_xes(path)\n",
    "\n",
    "def get_filtered_log(path, num_top_k):\n",
    "    filtered_log = get_log(path)\n",
    "    filtered_log = pm4py.filter_variants_top_k(filtered_log, num_top_k)\n",
    "\n",
    "    return filtered_log\n",
    "\n",
    "# fetch the information out of an event log using pm4py\n",
    "full_log = get_log(\"../logs/sepsis_event_log.xes\")\n",
    "filtered_log = get_filtered_log(\"../logs/sepsis_event_log.xes\", num_top_k=62)\n",
    "highly_filtered_log = get_filtered_log(\"../logs/sepsis_event_log.xes\", num_top_k=11)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f84e32d3",
   "metadata": {},
   "source": [
    "Different helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6596ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log_info(log, verbose=False):\n",
    "    '''\n",
    "    Prints out useful information of the given event log\n",
    "\n",
    "    Prints out all start activities, end activities, all event attributes, and all trace attributes.\n",
    "    If verbose is True, then all values of all event and trace attributes are printed out plus all variants of the event log\n",
    "    '''\n",
    "\n",
    "    start_activities = pm4py.stats.get_start_activities(log)\n",
    "    end_activities = pm4py.stats.get_end_activities(log)\n",
    "    event_attributes = pm4py.stats.get_event_attributes(log)\n",
    "    trace_attributes = pm4py.stats.get_trace_attributes(log)\n",
    "    \n",
    "    info = f\"Event log information:\\n\"\n",
    "    info += f\"Start activities: {start_activities}\\n\"\n",
    "    info += f\"End activities: {end_activities}\\n\"\n",
    "    info += f\"Event attributes: {event_attributes}\\n\"\n",
    "    info += f\"Trace attributes: {trace_attributes}\\n\"\n",
    "\n",
    "    # add additional info if wished for\n",
    "    if verbose:\n",
    "        event_attribute_values = [\n",
    "            {attribute: pm4py.stats.get_event_attribute_values(log, attribute)}\n",
    "            for attribute in event_attributes\n",
    "        ]\n",
    "        trace_attribute_values = [\n",
    "            {attribute: pm4py.stats.get_trace_attribute_values(log, attribute)}\n",
    "            for attribute in trace_attributes\n",
    "        ]\n",
    "        variants = pm4py.stats.get_variants(log)\n",
    "\n",
    "        info += f\"Event attribute values: {event_attribute_values}\\n\"\n",
    "        info += f\"Trace attribute values: {trace_attribute_values}\\n\"\n",
    "        info += f\"Variants: {variants}\"\n",
    "\n",
    "    print(info)\n",
    "\n",
    "def show_dfg_of_log(log):\n",
    "    '''\n",
    "    Displays the directly follows graph of an event log\n",
    "    '''\n",
    "\n",
    "    dfg, start_activities, end_activities = pm4py.discovery.discover_dfg(log, \n",
    "                                                                         case_id_key= \"case:concept:name\",\n",
    "                                                                         activity_key= \"concept:name\")\n",
    "\n",
    "    return pm4py.vis.view_dfg(dfg, start_activities, end_activities, format=\"png\")\n",
    "\n",
    "def get_df_from_dfg(log):\n",
    "    '''\n",
    "    Reads in the log and then produces a dfg from it. The function then generates a Pandas DataFrame.\n",
    "\n",
    "    Every row in the DataFrame has a source node, a target node and the frequency of the vertex\n",
    "    '''\n",
    "\n",
    "    dfg, _, _ = pm4py.discover_dfg(log)\n",
    "    connections_list = [[edges[0], edges[1], weight] for edges, weight in dfg.items()] \n",
    "    df = pd.DataFrame(data=connections_list, columns=[\"From\", \"To\", \"Frequency\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_pivot_df_from_dfg(log):\n",
    "    '''\n",
    "    Reads in the log and then produces a dfg from it. The function then generates a Pandas DataFrame.\n",
    "\n",
    "    The returned DataFrame is a pivot table, rows are source nodes, columns are target nodes and the values is the frequency the vertex has been used \n",
    "    '''\n",
    "\n",
    "    df = get_df_from_dfg(log)\n",
    "\n",
    "    # find events which are not in ingoing and outgoing events\n",
    "    from_events = df[\"From\"].unique()\n",
    "    to_events = df[\"To\"].unique()\n",
    "    only_from_elements = list(set(from_events) - set(to_events))\n",
    "    only_to_elements = list(set(to_events) - set(from_events))\n",
    "\n",
    "    # add them to the dataframe in the column in which they were not in\n",
    "    # that way all elements in the pivot table are in both column and rows\n",
    "    for element in only_from_elements:\n",
    "        df.loc[len(df)] = [from_events[0], element, 0]\n",
    "    for element in only_to_elements:\n",
    "        df.loc[len(df)] = [element, to_events[0], 0]\n",
    "\n",
    "    df = df.pivot(index=\"From\", columns=\"To\", values=\"Frequency\")\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_weighted_df(df):\n",
    "    '''\n",
    "    Transforms the frequency of the used vertices to weights. The weight is the portion of all outgoing vertices of a node.\n",
    "    Ex.: Node A has three vertices with frequency 8, 7 and 5. After applying the transformation the weights are 0.4, 0.35 and 0.25\n",
    "\n",
    "    Returns the normalized df\n",
    "    '''\n",
    "    \n",
    "    normalized_df = df.apply(lambda x: (x / x.sum()), axis=\"columns\")\n",
    "    normalized_df = normalized_df.fillna(0)\n",
    "\n",
    "    return normalized_df\n",
    "\n",
    "def get_jaccard_distance_of_dfg(log):\n",
    "    '''\n",
    "    Computes the jaccard measure of two nodes using the DFG\n",
    "\n",
    "    NetworkX provides an implementation of the jaccard measure in directly-follows-graphs.\n",
    "    First the DFG has to be generated using pm4py. The output gets formatted for a NetworkX graph.\n",
    "    Output is an iterator of the pairwise distances of the nodes in the graph\n",
    "    '''\n",
    "\n",
    "    # convert event log to an undirected NetworkX graph\n",
    "    dfg, start, end = pm4py.discover_dfg(log)\n",
    "    edge_list = [(edge[0], edge[1], weight) for edge, weight in dfg.items()]\n",
    "    dg = nx.DiGraph()\n",
    "    dg.add_weighted_edges_from(edge_list)\n",
    "\n",
    "    # compute the jaccard measures of the nodes\n",
    "    distances = nx.jaccard_coefficient(dg.to_undirected())\n",
    "\n",
    "    # print distances between the nodes\n",
    "    #   for distance in distances:\n",
    "    #        print(distance)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def calculate_weighted_jaccard_similarity(G, node1, node2):\n",
    "    '''\n",
    "    Calculates the weighted jaccard distance between two nodes of a Graph\n",
    "    Formula is the sum of the minimum weights of the union of vertices\n",
    "    divited by the sum of the maximum weights of the union of vertices\n",
    "\n",
    "    Source: https://stackoverflow.com/a/69218150\n",
    "    '''\n",
    "    \n",
    "    # skip calculating the similarity if the nodes to compare are the same\n",
    "    if node1 == node2:\n",
    "        return 1\n",
    "\n",
    "    neighbors1 = set(G.neighbors(node1))\n",
    "    neighbors2 = set(G.neighbors(node2))\n",
    "    minimums = 0\n",
    "    maximums = 0\n",
    "\n",
    "    for x in neighbors1.union(neighbors2):\n",
    "        node1_weight = 0\n",
    "        node2_weight = 0\n",
    "\n",
    "        if x in G[node1]:\n",
    "            node1_weight = G[node1][x]['weight']\n",
    "        if x in G[node2]:\n",
    "            node2_weight = G[node2][x]['weight']\n",
    "        \n",
    "        minimums += min(node1_weight, node2_weight)\n",
    "        maximums += max(node1_weight, node2_weight)\n",
    "    # prevent division by 0 error\n",
    "    if maximums == 0:\n",
    "        return 0\n",
    "    return minimums / maximums\n",
    "\n",
    "def get_weighted_jaccard_distance_matrix(connections_df):\n",
    "    '''\n",
    "    Computes the weighted jaccard distance for every node pair\n",
    "\n",
    "    Returns a pivoted DataFrame where the row is the source node,\n",
    "    the column the target node and the values are the distances between the nodes \n",
    "    '''\n",
    "    \n",
    "    G = nx.from_pandas_adjacency(connections_df, nx.DiGraph)\n",
    "    jaccard_table = {} # the distances between the nodes\n",
    "\n",
    "    for node1 in G.nodes:\n",
    "        node1_distances = {}\n",
    "        for node2 in G.nodes:\n",
    "            similarity = calculate_weighted_jaccard_similarity(G, node1, node2)\n",
    "            node1_distances[node2] = round(1 - similarity, 6)\n",
    "        jaccard_table[node1] = node1_distances\n",
    "\n",
    "    jaccard_table = pd.DataFrame(jaccard_table)\n",
    "    return jaccard_table\n",
    "\n",
    "def get_simrank_similarity_of_dfg(log):\n",
    "    '''\n",
    "    Computes the simrank similarity of two nodes using the DFG\n",
    "\n",
    "    Output is a dictionary of dictionaries with the first key being the source node, the key\n",
    "    in the second dictionary being the target node, and the value being the simrank similarity.\n",
    "    The distance relation between source and target are symmetrical. \n",
    "    '''\n",
    "\n",
    "    # convert event log to a NetworkX graph\n",
    "    dfg, start, end = pm4py.discover_dfg(log)\n",
    "    edge_list = [(edge[0], edge[1], weight) for edge, weight in dfg.items()]\n",
    "    dg = nx.DiGraph()\n",
    "    dg.add_weighted_edges_from(edge_list)\n",
    "\n",
    "    # compute the simrank similarity of the nodes\n",
    "    distances = nx.simrank_similarity(dg)\n",
    "\n",
    "    # print distances between the nodes\n",
    "    #   pprint.pprint(distances)\n",
    "\n",
    "    return distances\n",
    "\n",
    "def convert_jaccard_distances_to_distance_matrix(log, pairwise_distances):\n",
    "    '''\n",
    "    Converts the pairwise distances iterator from NetworkX to a distance matrix\n",
    "\n",
    "    First finds the attributes and generates an empty distance matrix.\n",
    "    The cells will be filled with 1 - the values of the corresponding jaccard measure.\n",
    "    Remaining cells have the distance 1\n",
    "    Returns the list of activities and the distance matrix\n",
    "    '''\n",
    "    \n",
    "    # retrieve all attribute values\n",
    "    activities = list(pm4py.get_event_attribute_values(log, \"concept:name\").keys())\n",
    "    activities.sort()\n",
    "\n",
    "    # create empty distance matrix using the activity names    \n",
    "    distance_matrix = pd.DataFrame(columns=activities, index=activities)\n",
    "\n",
    "    # fill matrix up\n",
    "    for u, v, distance in pairwise_distances:\n",
    "        distance_matrix[u][v] = 1 - round(distance, 3)\n",
    "        distance_matrix[v][u] = 1 - round(distance, 3)\n",
    "\n",
    "    distance_matrix = distance_matrix.fillna(1) # no similarity -> distance = 1\n",
    "    distance_matrix = distance_matrix.to_numpy()\n",
    "    np.fill_diagonal(distance_matrix, 0) # similarity to itself is always 0\n",
    "    \n",
    "    # return only the values of the matrix, not the names of the activities\n",
    "    return activities, distance_matrix\n",
    "\n",
    "def convert_simrank_distances_to_distance_matrix(log, pairwise_distances):\n",
    "    '''\n",
    "    Converts the pairwise distances dictionary of dictionaries to a distance matrix\n",
    "    '''\n",
    "\n",
    "    # create distance matrix using the pairwise distances dictionary and sort them\n",
    "    distance_matrix = pd.DataFrame(pairwise_distances)\n",
    "    distance_matrix = distance_matrix.reindex(sorted(distance_matrix.columns), axis=\"index\")\n",
    "    distance_matrix = distance_matrix.reindex(sorted(distance_matrix.columns), axis=\"columns\")\n",
    "    activities = distance_matrix.columns\n",
    "\n",
    "    # transform values and convert to numpy matrix\n",
    "    distance_matrix = distance_matrix.transform(lambda x: round(1 - x, 3))\n",
    "    print(distance_matrix)\n",
    "    distance_matrix = distance_matrix.to_numpy()\n",
    "    \n",
    "    # return only the values of the matrix, not the names of the activities\n",
    "    return activities, distance_matrix\n",
    "\n",
    "def create_linkage(distances):\n",
    "    '''\n",
    "    Performs hierarchical clustering using average linkage with scipy\n",
    "    Returns the linkage matrix\n",
    "    '''\n",
    "\n",
    "    # input for the method has to be a condensed distance matrix\n",
    "    condensed_matrix = squareform(distances)\n",
    "\n",
    "    # perform hierarchical clustering\n",
    "    return linkage(condensed_matrix, 'average')\n",
    "\n",
    "def create_dendrogram(activities, linkage_matrix):\n",
    "    '''\n",
    "    Creates the dendrogram of the hierarchical clustering operation using scipy\n",
    "    Displays the dendrogram to the terminal\n",
    "    '''\n",
    "\n",
    "    # create dendrogram out of data\n",
    "    dn = dendrogram(linkage_matrix,\n",
    "                    orientation=\"right\",\n",
    "                    labels=activities)\n",
    "\n",
    "    return dn\n",
    "\n",
    "def create_clusterings_for_every_level(activities, distances, linkage_matrix):\n",
    "    '''\n",
    "    Creates a dictionary for every clustering. Each entry includes a list of clusterings \n",
    "    for every level of the dendrogram.\n",
    "    Returns the clustering dictionary\n",
    "    '''\n",
    "\n",
    "    # output dictionary and the number of done merges \n",
    "    clusterings = {}\n",
    "    num_merges = 0\n",
    "\n",
    "    # add a singleton cluster for every activity at first entry in clusterings dictionary\n",
    "    dct = dict([(i, {activities[i]}) for i in range(distances.shape[0])])\n",
    "    clusterings[0] = list(dct.values())\n",
    "\n",
    "    # adapted from: https://stackoverflow.com/a/65060545\n",
    "    for i, row in enumerate(linkage_matrix, distances.shape[0]):\n",
    "        # add for every merge a union of the merged clusters and delete the old ones\n",
    "        dct[i] = dct[row[0]].union(dct[row[1]])\n",
    "        del dct[row[0]]\n",
    "        del dct[row[1]]\n",
    "        num_merges += 1\n",
    "\n",
    "        # save the clustering in the output dictionary\n",
    "        clusterings[num_merges] = list(dct.values())\n",
    "    \n",
    "    return clusterings\n",
    "\n",
    "def create_hierarchy_for_activities(activities, clusterings):\n",
    "    '''\n",
    "    Returns every cluster each activity was ever in. This in return generates the hierarchy needed\n",
    "    for abstraction of event logs\n",
    "    Hint: Currect runtime is O(|activities|^2), there maybe is a faster method, though this is fine for now\n",
    "    '''\n",
    "\n",
    "    # create empty hierarchy lists for each activity\n",
    "    hierarchies = {activity: [] for activity in activities}\n",
    "\n",
    "    # for each cluster in each level add the new cluster to the corresponding activities\n",
    "    for level in clusterings.values():\n",
    "        for cluster in level:\n",
    "            for activity in cluster:\n",
    "                act_hierarchy = hierarchies[activity]\n",
    "                # skip cluster if it is already in its hierarchy\n",
    "                if cluster in act_hierarchy:\n",
    "                    continue\n",
    "                act_hierarchy.append(cluster)\n",
    "\n",
    "    return hierarchies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_simrank(G, node1, node2, C=0.8, max_iterations=100, tolerance=1e-6):\n",
    "    if node1 == node2:\n",
    "        return 1.0\n",
    "\n",
    "    prev_sim = 0\n",
    "    sim = 1\n",
    "    iterations = 0\n",
    "\n",
    "    while abs(sim - prev_sim) > tolerance and iterations < max_iterations:\n",
    "        prev_sim = sim\n",
    "        neighbors1 = set(G.predecessors(node1))\n",
    "        neighbors2 = set(G.predecessors(node2))\n",
    "        sim = C / (len(neighbors1) * len(neighbors2)) * sum(G[node1][x]['weight'] * G[node2][y]['weight'] * weighted_simrank(G, x, y, C, max_iterations, tolerance) for x in neighbors1 for y in neighbors2)\n",
    "        iterations += 1\n",
    "\n",
    "    return sim\n",
    "\n",
    "def full_weighted_simrank(G):\n",
    "    similarities = {}\n",
    "    for node1 in G.nodes():\n",
    "        node_similarity = {}\n",
    "        for node2 in G.nodes():\n",
    "            node_similarity[node2] = weighted_simrank(G, node1, node2)\n",
    "        similarities[node1] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f92230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_clustering_using_weighted_jaccard(log):\n",
    "    connections_df = get_pivot_df_from_dfg(log)\n",
    "    connections_df = get_weighted_df(connections_df)\n",
    "\n",
    "    jaccard_distances = get_weighted_jaccard_distance_matrix(connections_df)\n",
    "    jaccard_activities = jaccard_distances.columns\n",
    "    display(jaccard_distances)\n",
    "    \n",
    "    jaccard_linkage = create_linkage(jaccard_distances.to_numpy())\n",
    "    result_dendrogram = create_dendrogram(jaccard_activities, jaccard_linkage)\n",
    "    \n",
    "    clusterings = create_clusterings_for_every_level(jaccard_activities, jaccard_distances, jaccard_linkage)\n",
    "    hierarchies = create_hierarchy_for_activities(jaccard_distances, clusterings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9fe5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "coselog = get_log(\"../logs/coselog.xes\")\n",
    "perform_clustering_using_weighted_jaccard(coselog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccfefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpic = get_log(\"../logs/BPI_Challenge_2013_incidents.xes\")\n",
    "perform_clustering_using_weighted_jaccard(bpic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb981b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "road_traffic = get_log(\"../logs/Road_Traffic_Fine_Management_Process.xes\")\n",
    "perform_clustering_using_weighted_jaccard(road_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a645f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_clustering_using_weighted_jaccard(full_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING\n",
    "import SimRank # imported from https://github.com/ysong1231/SimRank/tree/main\n",
    "\n",
    "connections_df_full = get_df_from_dfg(full_log)\n",
    "connections_df_filtered = get_df_from_dfg(filtered_log)\n",
    "#connections_df_full[\"Frequency\"] = connections_df_full.groupby(\"From\")[\"Frequency\"].transform(lambda x: (x / x.sum()))\n",
    "\n",
    "def perform_weighted_simrank(connections):\n",
    "    sr = SimRank.SimRank()\n",
    "    sr_distances = sr.fit(data=connections,\n",
    "                        verbose=False,\n",
    "                        weighted = True,\n",
    "                        from_node_column=\"From\",\n",
    "                        to_node_column=\"To\",\n",
    "                        weight_column=\"Frequency\")\n",
    "    sr_distances = sr_distances.transform(lambda x: 1 - x)\n",
    "    display(sr_distances)\n",
    "\n",
    "    sr_activities = sr_distances.columns\n",
    "    sr_distances = sr_distances.to_numpy()\n",
    "\n",
    "\n",
    "    # Apply min-max normalization to the entire array\n",
    "    np.fill_diagonal(sr_distances, np.nan)\n",
    "    min_val = np.nanmin(sr_distances)\n",
    "    max_val = np.nanmax(sr_distances)\n",
    "    normalized_data = np.vectorize(lambda x: (x - min_val) / (max_val - min_val))(sr_distances)\n",
    "    np.fill_diagonal(normalized_data, 0)\n",
    "    sr_distances = normalized_data\n",
    "\n",
    "    sr_linkage = create_linkage(sr_distances)\n",
    "    result_dendrogram = create_dendrogram(sr_activities, sr_linkage)\n",
    "\n",
    "#display(connections_df_full.pivot(index=\"From\", columns=\"To\", values=\"Frequency\").fillna(0))\n",
    "perform_weighted_simrank(connections_df_full)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "865f4474",
   "metadata": {},
   "source": [
    "Main program for executing the generalisation hierarchies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aacacd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_all_information_from_log(log):\n",
    "    # Compute the pairwise distances of the created DFG from the event log\n",
    "    # pairwise_distances = get_jaccard_distance_of_dfg(log)\n",
    "    pairwise_distances = get_simrank_similarity_of_dfg(log)\n",
    "\n",
    "    # displays the directly follows graph of the log\n",
    "    print(\"The generated directly-follows-graph of the log:\")\n",
    "    dfg = show_dfg_of_log(log)\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "    # Generate the distance matrix used for performing hierarchical clustering\n",
    "    # also prints the distance matrix while executing\n",
    "    print(\"Distances between the activities:\")\n",
    "    # activities, distances = convert_jaccard_distances_to_distance_matrix(log, pairwise_distances)\n",
    "    activities, distances = convert_simrank_distances_to_distance_matrix(log, pairwise_distances)\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n",
    "    # Perform hierarchical clustering using scipy\n",
    "    linkage_matrix = create_linkage(distances)\n",
    "\n",
    "    # create a dictionary for every clustering performed at each step\n",
    "    clusterings = create_clusterings_for_every_level(activities, distances, linkage_matrix)\n",
    "\n",
    "    # create the hierarchy needed for event log abstraction\n",
    "    print(\"The clusters each activity was in:\")\n",
    "    hierarchies = create_hierarchy_for_activities(activities, clusterings)\n",
    "    for activity, clusters in hierarchies.items():\n",
    "        print(f\"{activity}: \")\n",
    "        for cluster in clusters:\n",
    "            print(f\"\\t{cluster}\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    \n",
    "    # create a dendrogram for illustrative purposes\n",
    "    print(\"The resulting dendrogram of the activities:\")\n",
    "    result_dendrogram = create_dendrogram(activities, linkage_matrix)\n",
    "    print(\"------------------------------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65cc34f",
   "metadata": {},
   "source": [
    "The information of the full unfiltered log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe9f5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_information_from_log(full_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334fcc4b",
   "metadata": {},
   "source": [
    "The information from the filtered log (only variants which occur twice will be considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1633cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_information_from_log(filtered_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dc7b64",
   "metadata": {},
   "source": [
    "The information from the highly filtered log (only variants which occur five times will be considered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9bdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_information_from_log(highly_filtered_log)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
